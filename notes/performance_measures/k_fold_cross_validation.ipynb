{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold Cross-Validation\n",
    "<br>\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups\n",
    "3. For each unique group:\n",
    "4. Take the group as a hold out or test data set\n",
    "  1. Take the remaining groups as a training data set\n",
    "  2. Fit a model on the training set and evaluate it on the test set\n",
    "  3. Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.\n",
    "\n",
    "<br>\n",
    "The results of a k-fold cross-validation run are often summarized with the mean of the model skill scores. It is also good practice to include a measure of the variance of the skill scores, such as the standard deviation or standard error.\n",
    "\n",
    "Three common tactics for choosing a value for k are as follows:\n",
    "\n",
    "* Representative: The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
    "* k=10: The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.\n",
    "* k=n: The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called leave-one-out cross-validation.\n",
    "\n",
    "## Variations on Cross-Validation\n",
    "There are a number of variations on the k-fold cross validation procedure.\n",
    "\n",
    "Three commonly used variations are as follows:\n",
    "\n",
    "* Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
    "* LOOCV: Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.\n",
    "* Stratified: The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n",
    "* Repeated: This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample.\n",
    "\n",
    "The scikit-learn library provides a suite of cross-validation implementation. You can see the full list in the Model Selection API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)  \n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
